{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Computational Intelligence - PART-A",
   "id": "6a4e0f8e06d99d89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting Alzheimer's Disease using Neural Nets",
   "id": "de3f02886c82015c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "id": "7fd0c2845e33a0c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_dataset(filename):\n",
    "    try:\n",
    "        data = pd.read_csv(filename)\n",
    "        print(f\"Dataset loaded successfully: {filename}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found.\")"
   ],
   "id": "3b91b09fe263aab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()"
   ],
   "id": "ba3ce042c832c196",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### EDA",
   "id": "3b3febb8361b947f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_name = 'alzheimers_disease_data.csv'\n",
    "dataset = load_dataset(file_name)\n",
    "dataset.head()"
   ],
   "id": "1fbd2b99e3ac23d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Dataset Info:\")\n",
    "dataset.info()"
   ],
   "id": "cc4a1255275f31a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Summary Statistics\")\n",
    "dataset.describe()"
   ],
   "id": "d05a3249ca95cb2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_columns = [\n",
    "    'Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality',\n",
    "    'SleepQuality', 'SystolicBP', 'DiastolicBP', 'CholesterolTotal',\n",
    "    'CholesterolLDL', 'CholesterolHDL', 'CholesterolTriglycerides',\n",
    "    'MMSE', 'FunctionalAssessment', 'ADL'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'Gender', 'Ethnicity', 'EducationLevel', 'Smoking', 'FamilyHistoryAlzheimers',\n",
    "    'CardiovascularDisease', 'Diabetes', 'Depression', 'HeadInjury', 'Hypertension',\n",
    "    'MemoryComplaints', 'BehavioralProblems', 'Confusion', 'Disorientation',\n",
    "    'PersonalityChanges', 'DifficultyCompletingTasks', 'Forgetfulness', 'Diagnosis'\n",
    "]\n",
    "\n",
    "def plot_histograms_separate(dataset, numerical_columns):\n",
    "    for column in numerical_columns:\n",
    "        # Create a figure with 1 row and 2 columns\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "        # Left subplot: Overall distribution\n",
    "        sns.histplot(\n",
    "            data=dataset,\n",
    "            x=column,\n",
    "            color=\"blue\",\n",
    "            kde=True,\n",
    "            bins=15,\n",
    "            ax=axes[0]\n",
    "        )\n",
    "        axes[0].set_title(f'Overall Distribution of {column}')\n",
    "        axes[0].set_xlabel(column)\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "\n",
    "        # Right subplot: Distribution by diagnosis\n",
    "        sns.histplot(\n",
    "            data=dataset,\n",
    "            x=column,\n",
    "            hue=\"Diagnosis\",\n",
    "            palette=[\"#90CAF9\", \"#F8A170\"],\n",
    "            kde=True,\n",
    "            bins=15,\n",
    "            multiple=\"layer\",\n",
    "            ax=axes[1],\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "        handles, labels = axes[1].get_legend_handles_labels()\n",
    "        axes[1].legend(handles, ['No Alzheimer\\'s', 'Alzheimer\\'s'])\n",
    "\n",
    "        axes[1].set_title(f'Distribution of {column} by Diagnosis')\n",
    "        axes[1].set_xlabel(column)\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_boxplots(dataset):\n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    melted_data = pd.melt(dataset[numerical_columns])\n",
    "\n",
    "    # Create a single boxplot with all numerical columns\n",
    "    sns.boxplot(x='variable', y='value', data=melted_data, ax=ax)\n",
    "    ax.set_title('Boxplots of Numerical Features', fontsize=16)\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_crosstabs():\n",
    "    for column in categorical_columns:\n",
    "        if column != 'Diagnosis':  # Skip the Diagnosis column itself\n",
    "            # Create a figure with 1 row and 2 columns of subplots\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "            # Create crosstab between current feature and Diagnosis\n",
    "            crosstab = pd.crosstab(dataset[column], dataset['Diagnosis'])\n",
    "\n",
    "            if column == 'Gender':\n",
    "                crosstab.index = ['Male', 'Female']\n",
    "            elif column == 'Ethnicity':\n",
    "                crosstab.index = ['Caucasian', 'African American', 'Asian', 'Other']\n",
    "            elif column == 'EducationLevel':\n",
    "                crosstab.index = ['None', 'High School', 'Bachelor\\'s', 'Higher']\n",
    "            elif column in ['Smoking', 'FamilyHistoryAlzheimers', 'CardiovascularDisease',\n",
    "                          'Diabetes', 'Depression', 'HeadInjury', 'Hypertension',\n",
    "                          'MemoryComplaints', 'BehavioralProblems', 'Confusion',\n",
    "                          'Disorientation', 'PersonalityChanges',\n",
    "                          'DifficultyCompletingTasks', 'Forgetfulness']:\n",
    "                crosstab.index = ['No', 'Yes']\n",
    "\n",
    "            crosstab.columns = ['No Alzheimer\\'s', 'Alzheimer\\'s']\n",
    "\n",
    "            # Plot 1: Raw counts (left subplot)\n",
    "            ax1 = crosstab.plot(kind='bar', stacked=True, ax=axes[0])\n",
    "            axes[0].set_title(f'{column} vs. Diagnosis (Counts)', fontsize=14)\n",
    "            axes[0].set_xlabel(column, fontsize=12)\n",
    "            axes[0].set_ylabel('Count', fontsize=12)\n",
    "            axes[0].legend(title='Diagnosis')\n",
    "            axes[0].set_xticklabels(crosstab.index, rotation=0)\n",
    "\n",
    "            # Add value labels on the bars\n",
    "            for container in ax1.containers:\n",
    "                ax1.bar_label(container, label_type='center', fmt='%d')\n",
    "\n",
    "            # Plot 2: Normalized percentages (right subplot)\n",
    "            crosstab_norm = crosstab.div(crosstab.sum(axis=1), axis=0) * 100\n",
    "            ax2 = crosstab_norm.plot(kind='bar', stacked=True, ax=axes[1])\n",
    "            axes[1].set_title(f'{column} vs. Diagnosis (Normalized %)', fontsize=14)\n",
    "            axes[1].set_xlabel(column, fontsize=12)\n",
    "            axes[1].set_ylabel('Percentage (%)', fontsize=12)\n",
    "            axes[1].legend(title='Diagnosis')\n",
    "            axes[1].set_xticklabels(crosstab.index, rotation=0)\n",
    "\n",
    "            # Add percentage labels on the bars\n",
    "            for container in ax2.containers:\n",
    "                ax2.bar_label(container, label_type='center', fmt='%.1f%%')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ],
   "id": "6faf1d55b30259de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_histograms_separate(dataset, numerical_columns)",
   "id": "ec1c0f473b0e3fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_boxplots(dataset)",
   "id": "81c7611df6c6073b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_categorical_crosstabs()",
   "id": "8b98e0f0ec7e9292",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories = ['No', 'Yes']\n",
    "counts = dataset.Diagnosis.value_counts().tolist()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(counts, labels=categories, autopct='%1.1f%%')\n",
    "plt.title('Diagnosis Distribution')\n",
    "plt.show()"
   ],
   "id": "4d303b094f8dddb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A1: Data preprocessing and preparation",
   "id": "ca0d579b88dd908e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(dataset, target_column):\n",
    "    dataset = dataset.drop(columns=['PatientID', 'DoctorInCharge'])\n",
    "\n",
    "    # Split into features and target\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "\n",
    "    return X, y"
   ],
   "id": "bfff2440f6f80f3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_and_fit_preprocessors(X_train, features_to_encode, numerical_features):\n",
    "    # Create and fit encoders on training data\n",
    "    encoders = {}\n",
    "    for feature in features_to_encode:\n",
    "        encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "        encoder.fit(X_train[[feature]])\n",
    "        encoders[feature] = encoder\n",
    "\n",
    "    # Create and fit scalers on training data\n",
    "    c_scaler = None\n",
    "    mm_scaler = None\n",
    "    if numerical_features:\n",
    "        c_scaler = StandardScaler(with_std=False)  # Only center, do not standardize\n",
    "        c_scaler.fit(X_train[numerical_features])\n",
    "\n",
    "        mm_scaler = MinMaxScaler()\n",
    "        mm_scaler.fit(X_train[numerical_features])\n",
    "\n",
    "    return encoders, c_scaler, mm_scaler\n"
   ],
   "id": "a5f39cb2830d3e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## A2 - A5",
   "id": "54d9d5e5a6df4f21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1, activation='relu', dropout_rate=0.0):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Check input_sizes\n",
    "        if not isinstance(hidden_sizes, list) or len(hidden_sizes) == 0:\n",
    "            raise ValueError(\"Hidden sizes must be a non-emtpy list\")\n",
    "\n",
    "        if activation.lower() == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation.lower() == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation.lower() == 'silu':\n",
    "            self.activation = nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError(f'Activation {activation} is not supported.')\n",
    "\n",
    "        # Construct layers based on the size of the hidden_sizes list\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(self.activation)\n",
    "\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "12462eb21e0acf6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_hidden_sizes(input_size):\n",
    "    H1 = input_size // 2\n",
    "    H2 = 2 * input_size // 3\n",
    "    H3 = input_size\n",
    "    H4 = 2 * input_size\n",
    "    return [H1, H2, H3, H4]"
   ],
   "id": "e78641f8daf0578d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_model_instance(input_size, hidden_sizes, config):\n",
    "    # Create model instance\n",
    "    model = Net(input_size, hidden_sizes, activation=config['activation'], dropout_rate=config['dropout_rate'])\n",
    "\n",
    "    # Define loss function for binary classification\n",
    "    loss_fun = nn.BCELoss()\n",
    "\n",
    "    # Define optimizer\n",
    "    if 'weight_decay' in config:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "    elif 'momentum' in config:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    return model, loss_fun, optimizer"
   ],
   "id": "fa7c704ee099534c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model, train_loader, val_loader, loss_fun, optimizer, device, config, verbose=2):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_mse = []\n",
    "    val_mse = []\n",
    "\n",
    "    patience = config['patience']\n",
    "    counter = 0\n",
    "    best_model = None\n",
    "\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    model.to(device)\n",
    "    model.double()\n",
    "\n",
    "    epoch_range = tqdm(range(config['num_epochs'])) if verbose == 2 else range(config['num_epochs'])\n",
    "    for epoch in epoch_range:\n",
    "        # Train model\n",
    "        model.train()\n",
    "        batch_loss = 0.0\n",
    "        batch_mse = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Perform forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fun(outputs, labels)\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            batch_mse += mse_loss(outputs, labels).item() * inputs.size(0)\n",
    "\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Epoch metrics calculation\n",
    "        epoch_train_loss = batch_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        epoch_train_mse = batch_mse / len(train_loader.dataset)\n",
    "\n",
    "        # Epoch metrics storage\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        train_mse.append(epoch_train_mse)\n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "        batch_loss = 0.0\n",
    "        batch_mse = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = loss_fun(outputs, labels)\n",
    "                batch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                mse = mse_loss(outputs, labels)\n",
    "                batch_mse += mse.item() * inputs.size(0)\n",
    "\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Epoch metrics calculation for validation\n",
    "        epoch_val_loss = batch_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct / total\n",
    "        epoch_val_mse = batch_mse / len(val_loader.dataset)\n",
    "\n",
    "        # Epoch metrics storage for validation\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        val_mse.append(epoch_val_mse)\n",
    "\n",
    "        if verbose == 2:\n",
    "             print(f'Epoch {epoch+1}/{config[\"num_epochs\"]} - '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, Train MSE: {epoch_train_mse:.4f}, Train Acc: {epoch_train_acc:.4f} - '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f}, Val MSE: {epoch_val_mse:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "        elif verbose == 1 and ((epoch+1) % 5 == 0 or epoch == 0):\n",
    "            print(f'Epoch {epoch+1}/{config[\"num_epochs\"]} - '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f}, Train MSE: {epoch_train_mse:.4f}, Train Acc: {epoch_train_acc:.4f} - '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f}, Val MSE: {epoch_val_mse:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            counter = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'train_mse': train_mse,\n",
    "        'val_mse': val_mse,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }"
   ],
   "id": "6286f80bd29bad85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def apply_fold_preprocessing(X_train, X_val, y_train, y_val, features_to_encode, numerical_features, config):\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_val_processed = X_val.copy()\n",
    "\n",
    "    # Create and fit preprocessors using only training data\n",
    "    encoders, c_scaler, mm_scaler = create_and_fit_preprocessors(\n",
    "        X_train, features_to_encode, numerical_features\n",
    "    )\n",
    "\n",
    "    # Apply one-hot encoding\n",
    "    for feature in features_to_encode:\n",
    "        encoder = encoders[feature]\n",
    "\n",
    "        train_encoded = encoder.transform(X_train[[feature]])\n",
    "        val_encoded = encoder.transform(X_val[[feature]])\n",
    "\n",
    "        feature_names = []\n",
    "        categories = list(encoder.categories_[0])\n",
    "        for category in categories[1:]:\n",
    "            feature_names.append(f\"{feature}_{category}\")\n",
    "\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=feature_names, index=X_train.index)\n",
    "        val_encoded_df = pd.DataFrame(val_encoded, columns=feature_names, index=X_val.index)\n",
    "\n",
    "        X_train_processed = X_train_processed.drop(columns=[feature])\n",
    "        X_val_processed = X_val_processed.drop(columns=[feature])\n",
    "        X_train_processed = pd.concat([X_train_processed, train_encoded_df], axis=1)\n",
    "        X_val_processed = pd.concat([X_val_processed, val_encoded_df], axis=1)\n",
    "\n",
    "    if numerical_features and c_scaler and mm_scaler:\n",
    "        X_train_processed[numerical_features] = c_scaler.transform(X_train_processed[numerical_features])\n",
    "        X_val_processed[numerical_features] = c_scaler.transform(X_val_processed[numerical_features])\n",
    "\n",
    "        X_train_processed[numerical_features] = mm_scaler.transform(X_train_processed[numerical_features])\n",
    "        X_val_processed[numerical_features] = mm_scaler.transform(X_val_processed[numerical_features])\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_processed.values, dtype=torch.float64)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64).view(-1, 1)\n",
    "    X_val_tensor = torch.tensor(X_val_processed.values, dtype=torch.float64)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float64).view(-1, 1)\n",
    "\n",
    "    # Create tensor datasets\n",
    "    train_tensor_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_tensor_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_tensor_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_tensor_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    input_size = X_train_processed.shape[1]\n",
    "\n",
    "    return train_loader, val_loader, input_size"
   ],
   "id": "8b7407a34c9da1b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5-fold CV",
   "id": "733550310eb313a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_combined_training_plot(all_fold_metrics, title_str, param_str):\n",
    "    \"\"\"Create a combined plot of training loss across all folds\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Define colors for each fold\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "    # Plot training loss for each fold\n",
    "    for fold, metrics in enumerate(all_fold_metrics):\n",
    "        plt.plot(metrics['train_losses'], label=f'Fold {fold+1}', color=colors[fold % len(colors)])\n",
    "\n",
    "    plt.title(f\"Training Loss Across Folds - {title_str}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the combined plot\n",
    "    plt.savefig(f\"plots/combined_training_loss_{param_str}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def create_combined_validation_plot(all_fold_metrics, title_str, param_str):\n",
    "    \"\"\"Create a combined plot of validation loss across all folds\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Define colors for each fold\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "    # Plot validation loss for each fold\n",
    "    for fold, metrics in enumerate(all_fold_metrics):\n",
    "        plt.plot(metrics['val_losses'], label=f'Fold {fold+1}', color=colors[fold % len(colors)])\n",
    "\n",
    "    plt.title(f\"Validation Loss Across Folds - {title_str}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the combined plot\n",
    "    plt.savefig(f\"plots/combined_validation_loss_{param_str}.png\")\n",
    "    plt.close()\n"
   ],
   "id": "a54ad1f3967961a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_kfold_cv_for_param(k_fold, X_train_val, y_train_val, hidden_size, features_to_encode, numerical_features,\n",
    "                           config, device, experiment_type, verbose=1):\n",
    "    \"\"\"Helper function to run k-fold CV for a specific parameter configuration\"\"\"\n",
    "\n",
    "    # Initialize dictionaries to store results for this parameter value\n",
    "    fold_results = {\n",
    "        'ce_loss': [],\n",
    "        'mse': [],\n",
    "        'accuracy': [],\n",
    "        'best_val_loss': []\n",
    "    }\n",
    "\n",
    "    all_fold_metrics = []\n",
    "\n",
    "    title_str='--'\n",
    "    param_str='--'\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X_train_val, y_train_val)):\n",
    "        if verbose >= 1:\n",
    "            print(f\"\\n{'='*10} Fold {fold+1}/{config['n_folds']} {'='*10}\")\n",
    "\n",
    "        # Split the data for this fold\n",
    "        X_train_fold, X_val_fold = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "\n",
    "        # Apply preprocessing for this fold\n",
    "        train_loader, val_loader, _ = apply_fold_preprocessing(\n",
    "            X_train_fold, X_val_fold, y_train_fold, y_val_fold,\n",
    "            features_to_encode, numerical_features, config\n",
    "        )\n",
    "\n",
    "        # Create model instance\n",
    "        net, loss_fun, optimizer = create_model_instance(input_size, hidden_size, config)\n",
    "\n",
    "        # Train model\n",
    "        net, metrics = train(net, train_loader, val_loader, loss_fun, optimizer, device, config, verbose)\n",
    "\n",
    "        # Extract metrics\n",
    "        ce_loss = metrics['val_losses'][-1]\n",
    "        mse = metrics['val_mse'][-1]\n",
    "        accuracy = metrics['val_accuracies'][-1]\n",
    "        best_val_loss = metrics['best_val_loss']\n",
    "\n",
    "        if verbose >= 1:\n",
    "            print(f\"Fold {fold+1} Results - CE Loss: {ce_loss:.4f}, MSE: {mse:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Store results for this fold\n",
    "        fold_results['ce_loss'].append(ce_loss)\n",
    "        fold_results['mse'].append(mse)\n",
    "        fold_results['accuracy'].append(accuracy)\n",
    "        fold_results['best_val_loss'].append(best_val_loss)\n",
    "\n",
    "        all_fold_metrics.append(metrics)\n",
    "\n",
    "        # Generate plot title and filename\n",
    "        if experiment_type == 'activation':\n",
    "            param_str = f\"act_{config['activation']}\"\n",
    "            title_str = f\"Activation={config['activation']}, LR={config['learning_rate']}, Hidden Size={config['hidden_size']}\"\n",
    "        elif experiment_type == 'weight_decay':\n",
    "            param_str = f\"wd{config['weight_decay']}\"\n",
    "            title_str = f\"Weight Decay={config['weight_decay']}, Hidden Size={config['hidden_size']}, LR={config['learning_rate']}, Momentum={config['momentum']}, Activation={config['activation']}\"\n",
    "        elif experiment_type == 'lr_momentum':\n",
    "            param_str = f\"lr{config['learning_rate']}_mom{config['momentum']}\"\n",
    "            title_str = f\"LR={config['learning_rate']}, Momentum={config['momentum']}, Hidden Size={config['hidden_size']}, Activation={config['activation']}\"\n",
    "        elif experiment_type == 'deep_net':\n",
    "            # Deep network\n",
    "            param_str = f\"layers_{'x'.join(str(x) for x in hidden_size)}\"\n",
    "            title_str = f\"Layers={hidden_size}, Weight Decay={config['weight_decay']}, LR={config['learning_rate']}, Momentum={config['momentum']}, Activation={config['activation']}\"\n",
    "        elif experiment_type == 'hidden_size':\n",
    "            # Single hidden layer\n",
    "            h_size = hidden_size[0] if isinstance(hidden_size, list) else hidden_size\n",
    "            param_str = f\"h{h_size}\"\n",
    "            title_str = f\"Hidden Size={h_size}, LR={config['learning_rate']}\"\n",
    "\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Create combined plots after all folds are finished\n",
    "    create_combined_training_plot(all_fold_metrics, title_str, param_str)\n",
    "    create_combined_validation_plot(all_fold_metrics, title_str, param_str)\n",
    "\n",
    "    # Calculate average results across all folds\n",
    "    avg_results = {metric: np.mean(values) for metric, values in fold_results.items()}\n",
    "\n",
    "    return avg_results"
   ],
   "id": "39d623222f11282d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def stratified_k_fold(X_train_val, y_train_val, experiment_config, base_config, features_to_encode, numerical_features, verbose=2):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    os.makedirs(\"plots/\", exist_ok=True)\n",
    "\n",
    "    # Init stratified k-fold cross validation\n",
    "    k_fold = StratifiedKFold(n_splits=base_config['n_folds'], shuffle=True, random_state=42)\n",
    "\n",
    "    experiment_type = experiment_config['type']\n",
    "    experiment_results = {}\n",
    "\n",
    "    if experiment_type == 'hidden_size':\n",
    "        hidden_sizes = experiment_config['hidden_sizes']\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            if verbose >= 1:\n",
    "                 print(f\"\\n{'='*20} Testing hidden size: {hidden_size[0]} {'='*20}\")\n",
    "\n",
    "            # Run k-fold CV for this hidden size\n",
    "            results = run_kfold_cv_for_param(\n",
    "                k_fold, X_train_val, y_train_val, hidden_size=hidden_size,\n",
    "                features_to_encode=features_to_encode, numerical_features=numerical_features,\n",
    "                config=base_config, device=device, experiment_type=experiment_type, verbose=verbose\n",
    "            )\n",
    "\n",
    "            experiment_results[hidden_size[0]] = results\n",
    "\n",
    "    elif experiment_type == 'activation':\n",
    "        # For activation function experiments\n",
    "        activation_functions = experiment_config['functions']\n",
    "        hidden_size = experiment_config['hidden_size']\n",
    "\n",
    "        for activation in activation_functions:\n",
    "            if verbose >= 1:\n",
    "                print(f\"\\n{'='*20} Testing activation function: {activation} {'='*20}\")\n",
    "\n",
    "            # Create modified config with the activation function\n",
    "            temp_config = base_config.copy()\n",
    "            temp_config['activation'] = activation\n",
    "            temp_config['hidden_size'] = hidden_size\n",
    "\n",
    "            # Run k-fold CV for this activation function\n",
    "            results = run_kfold_cv_for_param(\n",
    "                k_fold, X_train_val, y_train_val, hidden_size=hidden_size,\n",
    "                features_to_encode=features_to_encode, numerical_features=numerical_features,\n",
    "                config=temp_config, device=device, experiment_type=experiment_type, verbose=verbose\n",
    "            )\n",
    "\n",
    "            experiment_results[activation] = results\n",
    "\n",
    "    elif experiment_type == 'lr_momentum':\n",
    "        # For lr_momentum experiments, we test specific combinations\n",
    "        param_combinations = experiment_config['combinations']\n",
    "        hidden_size = experiment_config['hidden_size']\n",
    "\n",
    "        for lr, momentum in param_combinations:\n",
    "            if verbose >= 1:\n",
    "                print(f\"\\n{'='*20} Testing lr={lr}, momentum={momentum} {'='*20}\")\n",
    "\n",
    "            # Create modified config for this combination\n",
    "            temp_config = base_config.copy()\n",
    "            temp_config['learning_rate'] = lr\n",
    "            temp_config['momentum'] = momentum\n",
    "            temp_config['hidden_size'] = hidden_size\n",
    "\n",
    "            # Run k-fold CV for this combination\n",
    "            results = run_kfold_cv_for_param(\n",
    "                k_fold, X_train_val, y_train_val, hidden_size=hidden_size,\n",
    "                features_to_encode=features_to_encode, numerical_features=numerical_features,\n",
    "                config=temp_config, device=device, experiment_type=experiment_type, verbose=verbose\n",
    "            )\n",
    "\n",
    "            experiment_results[(lr, momentum)] = results\n",
    "\n",
    "    elif experiment_type == 'weight_decay':\n",
    "        # For weight decay experiments\n",
    "        weigh_decay_values = experiment_config['weight_decay']\n",
    "        hidden_size = experiment_config['hidden_size']\n",
    "\n",
    "        learning_rate = experiment_config['learning_rate']\n",
    "        momentum = experiment_config['momentum']\n",
    "\n",
    "        for weight_decay in weigh_decay_values:\n",
    "            if verbose >= 1:\n",
    "                print(f\"\\n{'='*20} Testing weight decay: {weight_decay} {'='*20}\")\n",
    "\n",
    "\n",
    "\n",
    "            # Create modified config with the weight decay value\n",
    "            temp_config = base_config.copy()\n",
    "            temp_config['learning_rate'] = learning_rate\n",
    "            temp_config['momentum'] = momentum\n",
    "            temp_config['weight_decay'] = weight_decay\n",
    "            temp_config['hidden_size'] = hidden_size\n",
    "\n",
    "            # Run k-fold CV for this weight decay\n",
    "            results = run_kfold_cv_for_param(\n",
    "                k_fold, X_train_val, y_train_val, hidden_size=hidden_size,\n",
    "                features_to_encode=features_to_encode, numerical_features=numerical_features,\n",
    "                config=temp_config, device=device, experiment_type=experiment_type, verbose=verbose\n",
    "            )\n",
    "\n",
    "            experiment_results[weight_decay] = results\n",
    "\n",
    "    elif experiment_type == 'deep_net':\n",
    "        layer_configs = experiment_config['layer_configs']\n",
    "\n",
    "        learning_rate = experiment_config['learning_rate']\n",
    "        momentum = experiment_config['momentum']\n",
    "        weight_decay = experiment_config['weight_decay']\n",
    "\n",
    "        for layer_config in layer_configs:\n",
    "            config_name = \"x\".join([str(size) for size in layer_config])\n",
    "            if verbose >= 1:\n",
    "                print(f\"\\n{'='*20} Testing layer configuration: {config_name} {'='*20}\")\n",
    "\n",
    "            temp_config = base_config.copy()\n",
    "            temp_config['learning_rate'] = learning_rate\n",
    "            temp_config['momentum'] = momentum\n",
    "            temp_config['weight_decay'] = weight_decay\n",
    "\n",
    "            # Run k-fold CV for this layer configuration\n",
    "            results = run_kfold_cv_for_param(\n",
    "                k_fold, X_train_val, y_train_val,\n",
    "                hidden_size=layer_config,\n",
    "                features_to_encode=features_to_encode, numerical_features=numerical_features,\n",
    "                config=temp_config, device=device, experiment_type=experiment_type, verbose=verbose\n",
    "            )\n",
    "\n",
    "            experiment_results[tuple(layer_config)] = results\n",
    "\n",
    "\n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Summary Results Table - {experiment_type}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if experiment_type == 'lr_momentum':\n",
    "        print(f\"{'Learning Rate':<15} {'Momentum':<15} {'CE Loss':<15} {'MSE':<15} {'Accuracy':<15}\")\n",
    "        print(\"-\"*75)\n",
    "        for (lr, momentum) in experiment_results:\n",
    "            result = experiment_results[(lr, momentum)]\n",
    "            print(f\"{lr:<15} {momentum:<15} {result['ce_loss']:<15.4f} \"\n",
    "                  f\"{result['mse']:<15.4f} {result['accuracy']:<15.4f}\")\n",
    "    elif experiment_type == 'activation':\n",
    "        print(f\"{'Activation':<15} {'CE Loss':<15} {'MSE':<15} {'Accuracy':<15}\")\n",
    "        print(\"-\"*60)\n",
    "        for activation in experiment_results:\n",
    "            result = experiment_results[activation]\n",
    "            print(f\"{activation:<15} {result['ce_loss']:<15.4f} \"\n",
    "                  f\"{result['mse']:<15.4f} {result['accuracy']:<15.4f}\")\n",
    "    elif experiment_type == 'deep_net':\n",
    "        print(f\"{'Layer Config':<25} {'CE Loss':<15} {'MSE':<15} {'Accuracy':<15}\")\n",
    "        print(\"-\"*70)\n",
    "        for base_config in experiment_results:\n",
    "            result = experiment_results[base_config]\n",
    "            # Format the layer configuration as a string\n",
    "            config_str = \"x\".join([str(size) for size in base_config])\n",
    "            print(f\"{config_str:<25} {result['ce_loss']:<15.4f} \"\n",
    "                  f\"{result['mse']:<15.4f} {result['accuracy']:<15.4f}\")\n",
    "    else:\n",
    "        param_name = {'hidden_size': 'Hidden Size', 'weight_decay': 'Weight Decay'}[experiment_type]\n",
    "        print(f\"{param_name:<15} {'CE Loss':<15} {'MSE':<15} {'Accuracy':<15}\")\n",
    "        print(\"-\"*60)\n",
    "        for param in experiment_results:\n",
    "            result = experiment_results[param]\n",
    "            print(f\"{param:<15} {result['ce_loss']:<15.4f} \"\n",
    "                  f\"{result['mse']:<15.4f} {result['accuracy']:<15.4f}\")\n",
    "\n",
    "    return experiment_results"
   ],
   "id": "ff8244b96daa2281",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_column = 'Diagnosis'\n",
    "features_to_encode = ['Ethnicity']\n",
    "\n",
    "numerical_columns = [\n",
    "    'Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality',\n",
    "    'SleepQuality', 'SystolicBP', 'DiastolicBP', 'CholesterolTotal',\n",
    "    'CholesterolLDL', 'CholesterolHDL', 'CholesterolTriglycerides',\n",
    "    'MMSE', 'FunctionalAssessment', 'ADL'\n",
    "]\n",
    "\n",
    "config = {\n",
    "    # Core parameters\n",
    "    'n_folds': 5,\n",
    "    'batch_size': 16,\n",
    "    'input_size': 34,\n",
    "    'num_epochs': 600,\n",
    "    'patience': 10,\n",
    "\n",
    "    # Default hyperparameters (might be overridden in some experiments)\n",
    "    'learning_rate': 0.001,\n",
    "    'momentum': 0.0,\n",
    "    'activation': 'relu',\n",
    "    'weight_decay': 0.0,\n",
    "    'dropout_rate': 0.0\n",
    "}\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "# Preprocess dataset\n",
    "X, y = preprocess_dataset(dataset, target_column)\n",
    "\n",
    "input_size = config['input_size']\n",
    "\n",
    "hidden_size_config = {\n",
    "    'type': 'hidden_size',\n",
    "    'hidden_sizes': [[size] for size in get_hidden_sizes(input_size)] # List of single element lists\n",
    "}\n",
    "\n",
    "activation_config = {\n",
    "    'type': 'activation',\n",
    "    'functions': ['relu', 'tanh', 'silu'],\n",
    "    'hidden_size': [68]\n",
    "}\n",
    "\n",
    "lr_momentum_config = {\n",
    "    'type': 'lr_momentum',\n",
    "    'combinations': [\n",
    "        (0.001, 0.2),  # First combination: learning_rate=0.001, momentum=0.2\n",
    "        (0.001, 0.6),  # Second combination: learning_rate=0.001, momentum=0.6\n",
    "        (0.05, 0.6),   # Third combination: learning_rate=0.05, momentum=0.6\n",
    "        (0.1, 0.6)     # Fourth combination: learning_rate=0.1, momentum=0.6\n",
    "    ],\n",
    "    'hidden_size': [68]\n",
    "}\n",
    "\n",
    "weight_decay_config = {\n",
    "    'type': 'weight_decay',\n",
    "    'weight_decay': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': 0.001,\n",
    "    'momentum': 0.6,\n",
    "    'hidden_size': [68]\n",
    "}\n",
    "\n",
    "deep_net_config = {\n",
    "    'type': 'deep_net',\n",
    "    'layer_configs': [\n",
    "        # Two hidden layers\n",
    "        [input_size * 2, input_size * 2],                     # Same size\n",
    "        [input_size * 2, input_size],                  # Decreasing\n",
    "        [input_size, input_size * 2],                  # Increasing\n",
    "\n",
    "        # Three hidden layers\n",
    "        [input_size * 2, input_size * 2, input_size * 2],                   # Same size\n",
    "        [input_size * 2, input_size, input_size * 2 // 3],       # Decreasing\n",
    "        [input_size * 2 // 3, input_size, input_size * 2]      # Increasing\n",
    "    ],\n",
    "    'weight_decay': 0.0001,\n",
    "    'momentum': 0.6,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n",
    "exp_config = hidden_size_config\n",
    "\n",
    "experiment_results = stratified_k_fold(\n",
    "    X,\n",
    "    y,\n",
    "    exp_config,\n",
    "    config,\n",
    "    features_to_encode,\n",
    "    numerical_columns,\n",
    "    verbose=verbose  # Set verbosity level\n",
    ")"
   ],
   "id": "290f592c9b1a964a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
